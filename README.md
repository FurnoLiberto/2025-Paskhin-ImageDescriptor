# 2025-Paskhin-ImageDescriptor
# Image Captioning with Vision Encoder-Decoder

Проект по обучению модели описания изображений (Image Captioning) на датасете COCO 2014.

## Архитектура
Мы используем гибридную архитектуру на основе трансформеров:
- Encoder (Зрение): `google/vit-base-patch16-224-in21k` (Vision Transformer). Извлекает признаки из изображения.
- Decoder (Текст): `gpt2`. Генерирует текст на основе признаков от энкодера.

Библиотеки: `transformers`, `datasets`, `accelerate`.

## Данные
- Использован датасет COCO 2014.
- Применена фильтрация: оставлены только цветные (RGB) изображения.
- Для демонстрации работоспособности использована выборка из 50000 изображений.

## Результаты обучения
Обучение проводилось в течение 10 эпох.
- **Training Loss:** снизился с 3.91 до 1.55.
- **Eval ROUGE-1:** 0.26 (модель начала угадывать ключевые слова).
- **Eval BLEU:** ~0.0 (из-за малого размера выборки модель не выучила сложные грамматические конструкции для точного совпадения).

Графики обучения (Loss) доступны в папке `runs/` (TensorBoard).

## Как запустить

1. Подготовка данных:
  
   ```bash
   python dataloader.py --limit_data 50000 --save_path "./processed_data"

2. Обучение:
  
   ```bash
   python train.py --data_path "./processed_data" --epochs 10

3. Инференс (проверка):
  
   ```bash
   python inference.py --image_path "URL_КАРТИНКИ"


## Выводы
Эксперимент подтвердил работоспособность архитектуры ViT + GPT2.
Модель демонстрирует снижение функции потерь (Loss).
