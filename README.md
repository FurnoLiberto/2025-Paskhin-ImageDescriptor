# Обучение модели описания изображений

Проект по обучению модели описания изображений (Image Captioning) на датасете COCO 2014.

## Архитектура
Мы используем гибридную архитектуру на основе трансформеров:
- Encoder (Зрение): `google/vit-base-patch16-224-in21k` (Vision Transformer). Извлекает признаки из изображения.
- Decoder (Текст): `gpt2`. Генерирует текст на основе признаков от энкодера.

Библиотеки: `transformers`, `datasets`, `accelerate`.

## Данные
- Использован датасет COCO 2014.
- Применена фильтрация: оставлены только цветные (RGB) изображения.
- Для демонстрации работоспособности использована выборка из 2000 изображений.

- ## Как запустить
1. Подготовка данных:  
   ```bash
   python dataloader.py --limit_data 2000 --save_path "./processed_data"

2. Обучение:  
   ```bash
   python train.py --data_path "./processed_data" --epochs 10

3. Инференс (проверка):  
   ```bash
   python inference.py --image_path "URL_КАРТИНКИ"

## Результаты обучения
Обучение проводилось в течение 10 эпох.
- **Training Loss:** снизился с 3.81 до 0.9.
- **Eval Loss:** Снизился с 1.83 до 1.68 и возрос до 1.85
- **Eval ROUGE-2:** 0.005 (модель начала угадывать пары слова).
- **Eval BLEU:** ~0.0 (из-за малого размера выборки модель не выучила сложные грамматические конструкции для точного совпадения).

<img width="1119" height="392" alt="изображение" src="https://github.com/user-attachments/assets/df67679a-35b8-42b8-ab6e-e8fe5248db25" />

<img width="1180" height="394" alt="изображение" src="https://github.com/user-attachments/assets/9c530d85-b2af-42b3-9ab3-887421fcb847" />

<img width="1144" height="420" alt="изображение" src="https://github.com/user-attachments/assets/ed2a0b4f-06a3-49c6-a83a-1bb856391b48" />

<img width="1156" height="461" alt="изображение" src="https://github.com/user-attachments/assets/4d116c46-753a-4b52-9399-89cd1cb3a26c" />

<img width="357" height="357" alt="изображение" src="https://github.com/user-attachments/assets/32034338-c5f8-4eb2-897a-7780d70d0179" />

## Выводы

Данный эксперимент следует считать частично успешным, но неудачным с точки зрения достижения высоких метрик качества (в частности, BLEU). Модель продемонстрировала способность к обучению (loss снижался), однако замечено переобучение и итоговые описания остаются примитивными.

### Основные причины неудачи:

Недостаточный объем данных и переобучение. Ключевая проблема - модель обучалась всего на 2 000 изображениях, что составляет около 2% от полного датасета COCO. Этого недостаточно для того, чтобы модель выучила все разнообразие объектов, сцен и языковых конструкций.

Чрезмерная строгость метрики BLEU. Метрика BLEU-4 остается на нуле, потому что она требует *идеального совпадения последовательности из 4 слов подряд с эталонным описанием. На ранних этапах модель учится генерировать правильные ключевые слова (что подтверждается ненулевыми метриками `ROUGE`), но еще не способна строить грамматически безупречные предложения.

Недостаточная продолжительность обучения. 10 эпох на ограниченном датасете недостаточно для полной сходимости такой сложной архитектуры. Слои `cross-attention`, связывающие зрение и текст, требуют длительного "прогрева" для эффективной работы.

### План по улучшению модели

Масштабирование данных и обучения:
   - Использовать полный датасет COCO. Это главный шаг, который позволит модели увидеть достаточное разнообразие для обобщения.
   - Увеличить количество эпох до 20-30. Это даст модели достаточно времени для тонкой настройки весов и изучения сложных зависимостей.

Тюнинг гиперпараметров:
   - Использовать планировщик скорости обучения (например, `lr_scheduler_type="cosine"`), который будет плавно уменьшать learning rate. Это помогает модели более точно сойтись к оптимальному решению в конце обучения.
   - Экспериментировать с параметрами оптимизатора `AdamW` (например, `beta1`, `beta2`).

Эксперименты с архитектурой
   - Заменить ViT-Base на `microsoft/swin-base-patch4-window7-224-in22k` (Swin Transformer), который часто показывает лучшие результаты на задачах компьютерного зрения.
